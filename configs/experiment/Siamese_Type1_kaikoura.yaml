# @package _global_

# to execute this experiment run:
# python train.py experiment:example

defaults:
  - override /datamodule: siamese_datamodule.yaml
  - override /model: siamese_model.yaml
  - override /callbacks: siamese_callback.yaml
  - override /logger: wandb.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "Siamese_Type1_kaikora"

seed: 12345

trainer:
  min_epochs: 1
  max_epochs: 1000
  gradient_clip_val: 0.5
  log_every_n_steps: 500
  gpus: 2
  strategy: 'ddp'
  #resume_from_checkpoint: '/home/jupyter/deepslide/logs/experiments/runs/MAE_Downstream/2022-09-21_06-04-52/checkpoints/last.ckpt'

model:
  input_size: [2,128,128]
  embedding_size: 32
  decoder_depth: 1
  encoder_depth: 1
  base_lr: 0.001
  unet: True
  cnn: True
  decoder_channels: [32]
  
datamodule:
    data_dir: #add path to data here
    dict_dir: # dictionaries are located in the /data folder
    batch_size: 32
    num_workers: 8
    pin_memory: False
    input_channels: ['vh', 'vv'] #, 'los.rdr_0', 'los.rdr_1', 'topophase.cor_1', 'topophase.flat_imag', 'topophase.flat_real', 'dem']
    input_transforms: ['Log_transform','Standardize']
    num_time_steps: 1
    setting: 'pretraining'
    datasets: ['kaikoura']

logger:
  wandb:
    tags: ["${name}"]
    project: 'siamese_pretraining'       
