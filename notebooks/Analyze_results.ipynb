{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93a017f-b8a4-4234-a3fe-5bcf412a483f",
   "metadata": {},
   "source": [
    "# Notebook for evaluating Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f64fe-631a-4361-abc8-aaa82f6730a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6445cd-64bc-4f2a-89d7-48aa9a1330f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics import AveragePrecision,Accuracy\n",
    "import torch\n",
    "### adding model to path\n",
    "sys.path.append('/home/jupyter/deepslide')\n",
    "from src.datamodules.siamese_datamodule import Siamese_Landslide_Datamodule\n",
    "from src.models.siamese_downstream_module import Segmentation_Model\n",
    "from src.models.siamese_module import Siamese_Type_1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3049679-87c0-43f0-93e7-bf30435943a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba01490-8a35-4ede-a372-2c1fee8345a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.special.expit(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053f3ca-3cbd-4e85-896f-6ffb4777d777",
   "metadata": {},
   "source": [
    "### choose which experiment to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b67ed-cd4f-49a7-8b5b-c6c2a7d60054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "dataset         = 'Hokkaido'\n",
    "pretraining     = 'Hokkaido'\n",
    "# same as dataset but in small caption\n",
    "dataset2        = 'hokkaido'  \n",
    "loss            = 'dice'\n",
    "experiment_name = 'segment_hokk_pretrain_hokk_cnn'\n",
    "trainsize       = '5'\n",
    "unet            = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e312b2-97b7-4663-a489-4704c1c23e44",
   "metadata": {},
   "source": [
    "### you will need to adapt the model paths here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8107b-af01-41c7-9590-b32b80685907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained models\n",
    "net = Siamese_Type_1(**{'input_size':[2,128,128],'embedding_size':32,'unet':True,'decoder_depth':1, 'encoder_depth':1, 'cnn':True,'base_lr':0.001,'decoder_channels':[32]})\n",
    "if pretraining=='Hokkaido':\n",
    "    pretrain_path = '/home/jupyter/deepslide/logs/experiments/runs/Siamese_Type1_hokkaido/2022-09-27_12-14-28/checkpoints/epoch_269.ckpt'\n",
    "elif pretraining==\"Kaikoura\":\n",
    "    pretrain_path = '/home/jupyter/deepslide/logs/experiments/runs/Siamese_Type1_kaikora/2022-09-27_11-38-01/checkpoints/last.ckpt'\n",
    "   \n",
    "path = Path('/home/jupyter/deepslide/logs/experiments/runs/{}/'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f6d87-f4fc-40ea-9221-36ae8fa71726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, targets, threshold=0.5):\n",
    "    #print(preds)\n",
    "    preds          =preds.view((preds.size()[0],-1))\n",
    "    targets        =targets.view((targets.size()[0],-1))\n",
    "\n",
    "    prec, rec      = precision_recall(preds, targets, threshold=threshold)\n",
    "    f1_score       = 2*(prec*rec)/(prec+rec)\n",
    "\n",
    "    # pr_curve       = PrecisionRecallCurve(num_classes=5)\n",
    "    # precision, recall, thresholds = pr_curve(preds, targets)\n",
    "\n",
    "    average_precision = AveragePrecision()\n",
    "    AP_score       = average_precision(preds, targets)\n",
    "    accuracy       = Accuracy(threshold=threshold).cuda()\n",
    "    acc            = accuracy(preds, targets)\n",
    "    return f1_score.item(), AP_score.item(), prec.item(), rec.item(), acc.detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74855a47-7656-4949-9383-93a2bc151b28",
   "metadata": {},
   "source": [
    "### finds the right model files, evluates them, computes scores and dumps results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3a94c-fa0a-42ee-ae78-346defb18977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "filepaths1=[]\n",
    "filepaths2=[]\n",
    "for filepath in path.rglob(\"*config_tree.log\"):\n",
    "    file = open(filepath, 'r')\n",
    "    lines = file.read().splitlines()\n",
    "    for line in lines:\n",
    "        if 'loss:' in line:\n",
    "            if \" \"+loss in str(line):\n",
    "                filepaths1.append(filepath.parent)\n",
    "\n",
    "print(len(filepaths1))\n",
    "\n",
    "for pp in filepaths1:\n",
    "    for filepath in pp.glob(\"config_tree.log\"):\n",
    "        file = open(filepath, 'r')\n",
    "        lines = file.read().splitlines()\n",
    "        for line in lines:\n",
    "            if 'pretrain_path:' in line:\n",
    "                if pretrain_path in line:\n",
    "                    filepaths2.append(filepath.parent)\n",
    "\n",
    "for trainsize in ['2','5','10','20','-1']:\n",
    "    model_dirs_augmented = []\n",
    "    model_dirs = []\n",
    "    timestamps_augmented =[]\n",
    "    timestamps = []\n",
    "\n",
    "\n",
    "    filepaths3=[]\n",
    "\n",
    "    print(len(filepaths2))            \n",
    "    for pp in filepaths2:\n",
    "        for filepath in pp.rglob(\"*config_tree.log\"):\n",
    "            file = open(filepath, 'r')\n",
    "            lines = file.read().splitlines()\n",
    "            for line in lines:\n",
    "                if 'trainsize' in line:\n",
    "                    if trainsize in line:\n",
    "                        filepaths3.append(filepath.parent)\n",
    "\n",
    "    print(len(filepaths3))\n",
    "\n",
    "    count1, count2= 0,0\n",
    "    gss=[]\n",
    "\n",
    "    for pp in filepaths3:\n",
    "        count1+=1\n",
    "        for filepath in pp.glob(\"*config_tree.log\"):\n",
    "            file = open(filepath, 'r')\n",
    "            lines = file.read().splitlines()\n",
    "            for line in lines:\n",
    "                if '- hokkaido' in line:\n",
    "                    count2+=1\n",
    "                pp = filepath.parent/'checkpoints/'\n",
    "                if 'pre_train_augmented' in line:\n",
    "                    if 'true' in line:\n",
    "                        if len(list(pp.glob(\"*.ckpt\")))>3:\n",
    "                            model_dirs_augmented.append([file for file in pp.glob(\"*.ckpt\")])\n",
    "                            with open(filepath.parent/'wandb/latest-run/files/wandb-summary.json') as user_file:\n",
    "                                file_contents = user_file.read()\n",
    "                                timestamps_augmented.append(dict(json.loads(file_contents))['_timestamp'])\n",
    "                    else:\n",
    "                        with open(filepath.parent/'wandb/latest-run/files/wandb-summary.json') as user_file:\n",
    "                            file_contents = user_file.read()\n",
    "                            timestamps.append(dict(json.loads(file_contents))['_timestamp'])\n",
    "                        if len(list(pp.glob(\"*.ckpt\")))>3:\n",
    "                            model_dirs.append([file for file in pp.glob(\"*.ckpt\")])\n",
    "    print(len(model_dirs_augmented), len(model_dirs))                      \n",
    "    if trainsize=='2':\n",
    "        model_dirs_augmented = np.asarray(model_dirs_augmented)[np.argsort(np.asarray(timestamps_augmented))[-5::]].flatten()\n",
    "        model_dirs = np.asarray(model_dirs)[np.argsort(np.asarray(timestamps))[-5::]].flatten()\n",
    "    else:\n",
    "        model_dirs_augmented = np.asarray(model_dirs_augmented)[np.argsort(np.asarray(timestamps_augmented))[-3::]].flatten()\n",
    "        model_dirs = np.asarray(model_dirs)[np.argsort(np.asarray(timestamps))[-3::]].flatten()\n",
    "    print(len(model_dirs_augmented), len(model_dirs)) \n",
    "\n",
    "    model_dirs\n",
    "\n",
    "    datadict = {'data_dir': '/home/jupyter/deepslide/data/',\n",
    "        'dict_dir': '/home/jupyter/deepslide/data/',\n",
    "        'batch_size': 32,\n",
    "        'num_workers': 8,\n",
    "        'pin_memory': False,\n",
    "        'input_channels': ['vh', 'vv'], #, 'los.rdr_0', 'los.rdr_1', 'topophase.cor_1', 'topophase.flat_imag', 'topophase.flat_real', 'dem']\n",
    "        'input_transforms': ['Log_transform','Standardize'],\n",
    "        'num_time_steps': 1,\n",
    "        'setting': 'downstream',\n",
    "        'datasets': [dataset2]}\n",
    "\n",
    "    dloader = Siamese_Landslide_Datamodule(**datadict)\n",
    "    test_loader = dloader.test_dataloader()\n",
    "\n",
    "\n",
    "    # from collections import OrderedDict\n",
    "    # new_dict=OrderedDict()\n",
    "    # for key in pretrain_dict.keys():\n",
    "    #     new_dict['pretrained.'+key] = pretrain_dict[key]\n",
    "\n",
    "    mean_predictions={}\n",
    "    targets={}\n",
    "    metrics ={}   \n",
    "    for name, pre_train_augmented, paths in zip(['pretrain','no_pretrain'],[True, False],[model_dirs_augmented, model_dirs]):\n",
    "\n",
    "        metrics[name]={}\n",
    "        mean_predictions[name]={}\n",
    "        mean_predictions[name]['all']=np.zeros((56,128,128))\n",
    "        mean_predictions[name]['nl']=np.zeros((24,128,128))\n",
    "        mean_predictions[name]['l']=np.zeros((32,128,128))\n",
    "        metrics[name]['APRC'] = []\n",
    "\n",
    "\n",
    "        downstream_model_dict={    \n",
    "            'input_size': [4,128,128],\n",
    "            'embedding_size': 64,\n",
    "            'pre_train_augmented': pre_train_augmented,\n",
    "            'pretrain_path': pretrain_path,\n",
    "            'unet': unet,\n",
    "            'base_lr': 0.001,\n",
    "            'pretrain_params': {'input_size':[2,128,128],'embedding_size':32,'decoder_depth':1,'encoder_depth':1,'unet':True,'cnn':True,\n",
    "                                'base_lr':0.001,'decoder_channels':[32]},\n",
    "            'encoder_depth': 1,\n",
    "            'decoder_channels': [32],\n",
    "            'loss': loss}\n",
    "\n",
    "        count=0\n",
    "        model = Segmentation_Model(**downstream_model_dict)\n",
    "        #print(model)\n",
    "        for path in paths:\n",
    "            if not path.name=='last.ckpt':\n",
    "                if 'ap' in path.name:\n",
    "                    checkpoint    = torch.load(path)['state_dict']\n",
    "                    # for key in checkpoint.keys():\n",
    "                    #     print(key, checkpoint[key].shape)\n",
    "                    # pretrain_dict = torch.load(downstream_model_dict['pretrain_path'])['state_dict']\n",
    "                    # checkpoint.update(new_dict)\n",
    "                    model.load_state_dict(checkpoint)\n",
    "                    model.eval()\n",
    "                    model.cuda()\n",
    "                    for pre, post, label, names, weight in test_loader:\n",
    "                        with torch.no_grad():\n",
    "                            count+=1\n",
    "                            preds  = model.forward(pre.cuda(),post.cuda())\n",
    "                            label  = label.view(preds.size()).cuda()\n",
    "                            mean_predictions[name]['all']+=np.squeeze(preds.detach().cpu().numpy())\n",
    "                            targets['all'] = label.detach().cpu().numpy()\n",
    "                            landslide=torch.sum(label, axis=(1,2,3))>0\n",
    "\n",
    "                            preds_nl      = preds[~landslide]\n",
    "                            targets['nl'] = label[~landslide].detach().cpu().numpy()\n",
    "                            mean_predictions[name]['nl']+=np.squeeze(preds_nl.detach().cpu().numpy())\n",
    "                            \n",
    "                            preds_l      = preds[landslide]\n",
    "                            targets['l'] = label[landslide].detach().cpu().numpy()\n",
    "                            mean_predictions[name]['l']+=np.squeeze(preds_l.detach().cpu().numpy())\n",
    "                            \n",
    "                            preds    =torch.sigmoid(preds)\n",
    "\n",
    "                            res  = compute_metrics(preds, label.cuda(), threshold=0.5)\n",
    "  \n",
    "\n",
    "                            metrics[name]['APRC'].append(res[1])\n",
    "            if name=='pretrain':\n",
    "                save_model=model\n",
    "\n",
    "                            \n",
    "    results={}\n",
    "    results[dataset] = {}\n",
    "    \n",
    "    for name in ['pretrain','no_pretrain']:\n",
    "        results[dataset][name] = {}\n",
    "        for subfix in ['all','nl','l']:\n",
    "            results[dataset][name]['APRC_'+subfix]=[]\n",
    "            results[dataset][name]['f1_'+subfix]=[]\n",
    "            results[dataset][name]['iou_'+subfix]=[]\n",
    "            for ii in range(len(mean_predictions[name][subfix])):\n",
    "                preds = scipy.special.expit(mean_predictions[name][subfix]/count)[ii:ii+1]\n",
    "                res   = compute_metrics(torch.tensor(preds).cuda(), torch.tensor(targets[subfix]).cuda()[ii], threshold=0.5)\n",
    "                results[dataset][name]['APRC_'+subfix].append(res[1])\n",
    "                results[dataset][name]['f1_'+subfix].append(res[0])\n",
    "                intersection = np.logical_and(preds>0.5,targets[subfix][ii])\n",
    "                union        = np.logical_or(preds>0.5,targets[subfix][ii])\n",
    "                iou_score    = np.sum(intersection) / np.sum(union)\n",
    "                results[dataset][name]['iou_'+subfix].append(iou_score)\n",
    "                \n",
    "            results[dataset][name]['APRC_'+subfix]=np.asarray(results[dataset][name]['APRC_'+subfix])\n",
    "            results[dataset][name]['f1_'+subfix]=np.asarray(results[dataset][name]['f1_'+subfix])\n",
    "            results[dataset][name]['iou_'+subfix]=np.asarray(results[dataset][name]['iou_'+subfix])\n",
    "            \n",
    "\n",
    "            results[dataset][name]['APRC_'+subfix+'_mean']=np.mean(metrics[name]['APRC'])\n",
    "            results[dataset][name]['f1_'+subfix+'_mean']=np.mean(results[dataset][name]['f1_'+subfix])\n",
    "            results[dataset][name]['iou_'+subfix+'_mean']=np.mean(results[dataset][name]['iou_'+subfix])\n",
    "\n",
    "            results[dataset][name]['APRC_'+subfix+'_median']=np.median(metrics[name]['APRC'])\n",
    "            results[dataset][name]['f1_'+subfix+'_median']=np.median(results[dataset][name]['f1_'+subfix])\n",
    "            results[dataset][name]['iou_'+subfix+'_median']=np.median(results[dataset][name]['iou_'+subfix])\n",
    "\n",
    "            results[dataset][name]['APRC_'+subfix+'_std']=np.std(metrics[name]['APRC'])/len(metrics[name]['APRC'])\n",
    "            results[dataset][name]['f1_'+subfix+'_std']=np.std(results[dataset][name]['f1_'+subfix])/len(results[dataset][name]['f1_'+subfix])\n",
    "            results[dataset][name]['iou_'+subfix+'_std']=np.std(results[dataset][name]['iou_'+subfix])/len(results[dataset][name]['iou_'+subfix])\n",
    "\n",
    "\n",
    "            mean_predictions[name][subfix]=np.squeeze(scipy.special.expit(mean_predictions[name][subfix]/count))\n",
    "            results[dataset][name]['l1_'+subfix+'_mean'] = np.mean(np.abs(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3))))\n",
    "            results[dataset][name]['sl1_'+subfix+'_mean']= np.mean(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3)))\n",
    "\n",
    "            results[dataset][name]['l1_'+subfix+'_median'] = np.median(np.abs(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3))))\n",
    "            results[dataset][name]['sl1_'+subfix+'_median']= np.median(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3)))\n",
    "\n",
    "            results[dataset][name]['l1_'+subfix+'_std']  = np.std(np.abs(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3))))/len(targets[subfix])\n",
    "            results[dataset][name]['sl1_'+subfix+'_std'] = np.std(np.sum(mean_predictions[name][subfix], axis=(1,2))-np.sum(targets[subfix], axis=(1,2,3)))/len(targets[subfix])\n",
    "\n",
    "    pickle.dump(results, open('scores_{}_{}.pkl'.format(experiment_name, trainsize), 'wb'))\n",
    "\n",
    "    pickle.dump(mean_predictions,open('label_predictions_{}_{}.pkl'.format(experiment_name, trainsize), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "deepslide",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "deepslide",
   "language": "python",
   "name": "deepslide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
